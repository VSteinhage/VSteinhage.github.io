 <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<meta http-equiv="Content-Type" content="text-html" />
    <title>Informatik 4 - Intelligent Vision Systems Group</title>
    <!-- Hier das zust&auml;ndinge CSS-->
    <link href="../CSS/style.css" rel="stylesheet" type="text/css" media="screen" />
    <!--[if lte IE 6]>
  <style type="text/css">
   @import url(../CSSie5-6.css);
   </style>
   <![endif]-->
</head>

<body id="teaching">

<div id="wrapper">

<!-- ============================================================================ -->

    <div id="kopfbereich">
		<a href="http://net.cs.uni-bonn.de/wg/intelligent-vision-systems/">
            	   <h3><img id="logogruppe" src="../Images/Logos/IVS-4-100.png" height="50" /></a></h3>
<!--
		<a href="http://net.cs.uni-bonn.de/wg/intelligent-vision-systems/">
            	   <p>Intelligent Vision Systems Group</p></a>
-->
		<a href="http://net.cs.uni-bonn.de/start/">
		   <h3><img id="logo_cs4" src="../Images/Logos/logo_cs4_100.png" height="50" /></a></h3>
    </div><!--End kopfbereich-->

<!-- ============================================================================ -->

 <div id="inhalt">

<h3>BA-INF 051 - Projektgruppe Intelligente Sehsysteme</h3>

    <ul class="pre_teaching">
	    <li class="no_ein"><p>PD Dr. Volker Steinhage</p></li>
	    <li class="no_ein"><p>Dienstags, 14-16 Uhr, Seminarraum 1.012</p></li>
	    <li class="no_ein"><p>Vorbesprechung: Di, 13.09.22, 14 Uhr, via <a href="https://bbb.informatik.uni-bonn.de/b/vol-ung-npn">https://bbb.informatik.uni-bonn.de/b/vol-ung-npn</a>
<!--						<script TYPE="text/javascript" LANGUAGE="JavaScript">
							<!-- den Code vor Browsern verstecken, die kein JavaScript koennen
							var prefix = 'steinhage'; var domain = 'cs.uni-bonn.de';
							document.write('<a href=mailto:' + prefix + '@' + domain + '>');
							document.write(prefix + '@' + domain + '</'+'a>');
							// Ende des Versteckens vor alten Browsern -->
						

		</p></li>
    </ul>
<br>

<!-- ============================================================================ -->
<p>
<b>Themen</b>:
<!-- ============================================================================ -->

<!-- ============================================================================ -->
</p><p>
</p><table id="Feb2017" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
 <td>
	<div>
 	<img src="../Images/Labs/rgbd_presence_absence-v2.png" style="display:block" alt="" width="300" border="0">
	</div>
 </td>
 <td width="40"></td>
 <td> <p align="justify">
      <font face="arial,helvetica,sans-serif">
      <b>Animal Presence-Absence mittels RGB-D-Daten</b></font></p><font face="arial,helvetica,sans-serif">
<!--  ============================================================================ -->
      </font>
      <p align="justify">
      <font face="arial,helvetica,sans-serif">
	Zur Beobachtung von Tieren werden häufig Kamerafallen eingesetzt, die mittels Bewegungsmelder auslösen. 
	Diese Bewegungsmelder lösen jedoch auch häufig ohne Bewegung aus, was zu viel überflüssigem Bildmaterial führt. 
	Hier soll dieses überflüssige Bildmaterial aussortiert werden. 
	Dazu soll jedes Bild mittels des Deep-Learning-Ansatzes zur Knowledge Distillation (<a href="https://arxiv.org/abs/1503.02531">Hinton et al. 2015</a>) als “enthält Tier” oder “enthält kein Tier” klassifiziert werden. 
	Dabei steht als Eingabe nicht nur ein reguläres Intensitätsbild, sondern auch ein Tiefenbild, welches für jeden Pixel die Distanz zur Kamera kodiert, zur Verfügung. 
	Es soll evaluiert werden, inwiefern die zusätzlichen Tiefeninformationen zu einer Verbesserung der Erkennung beitragen. 
	Als “Teacher” im Rahmen der Knowledge Distillation soll der “MegaDetector” (<a href="https://arxiv.org/abs/1907.06772">Beery et al. 2019</a>) dienen.
	</font></p>
      </font>
      <p align="justify">
      <font face="arial,helvetica,sans-serif">
 </font></p></td>
 </tr>
</tbody></table>
<br><br>

<!-- ============================================================================ -->
<p>
</p><table id="Feb2017" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
 <td>
	<div>
 	<img src="../Images/Labs/ir2rgb-v2.png" style="display:block" alt="" width="300" border="0">
	</div>
 </td>
 <td width="40"></td>
 <td> <p align="justify">
      <font face="arial,helvetica,sans-serif">
      <b>Kolorierung von Nah-Infrarotbildern</b></font></p><font face="arial,helvetica,sans-serif">
<!--  ============================================================================ -->
      </font>
      <p align="justify">
      <font face="arial,helvetica,sans-serif">
		Wild- und Überwachungskameras verwenden häufig unsichtbare Infrarotbeleuchtung und monochrome Bildsensoren um bei Nacht zumindest Grauwertbilder aufnehmen zu können. Für viele anschließende Aufgaben wie Objekterkennung sind Farbbilder allerdings besser geeignet. Daher gibt es bereits diverse, oft auf Deep Learning basierende Ansätze, um solche Aufnahmen zu kolorieren (<a href="https://arxiv.org/abs/1604.02245">Limmer and Lensch 2016</a>, <a href="https://waseda.pure.elsevier.com/en/publications/infrared-image-colorization-using-a-s-shape-network">Dong et al. 2018</a>, <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/PBVS/Mehri_Colorizing_Near_Infrared_Images_Through_a_Cyclic_Adversarial_Approach_of_CVPRW_2019_paper.pdf">Mehri and Sappa 2019</a>). Diese Ansätze sollen vergleichend Evaluiert und auf die Kolorierung von Wildtieraufnahmen angepasst werden. Hierfür sind gute Python-Kenntnisse erforderlich und Erfahrungen mit PyTorch vorteilhaft.
	  </font></p><p></p><font face="arial,helvetica,sans-serif">

	Bildquelle: <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/PBVS/Mehri_Colorizing_Near_Infrared_Images_Through_a_Cyclic_Adversarial_Approach_of_CVPRW_2019_paper.pdf">Mehri and Sappa 2019</a>

      </font>
      <p align="justify">
      <font face="arial,helvetica,sans-serif">
 </font></p></td>
 </tr>
</tbody></table>
<br><br>

<!-- ============================================================================ -->

<p>
</p><table id="Feb2017" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
 <td>
 	<img src="../Images/Labs/PGInstSegAbb.jpg" alt="" width="300" border="0">
 </td>
 <td width="40"></td>
 <td> <p align="justify">
      <font face="arial,helvetica,sans-serif">
      <b>Instance Segmentation (2 Themen / 2 Projektgruppenteilnehmende)</b></font></p><font face="arial,helvetica,sans-serif">
<!-- ============================================================================ -->
      </font>
      <p align="justify">
		  <font face="arial,helvetica,sans-serif">
		  	Ziel dieser Projektgruppe ist die Untersuchung eines Instance Segmentation Verfahrens, das auf Künstlichen Neuronalen Netzen basiert. Die Aufgabe einer Instance Segmentation ist es, Objekte in einem Bild oder Video zu detektieren und den genauen Umriss, die Instance Mask dieses Objektes, zu bestimmen. So können Individuen besser voneinander unterschieden werden.
			  <br />
		  Der ausgewählte Ansatz wird auf Trainingsdaten aus einem aktuellen Forschungsprojekt angewendet. Die Implementierung wird mit Python und PyTorch durchgeführt. Neben der Implementierung des Instance Segmentation Verfahrens wird es eine Einarbeitungsphase in die Annotation von Datensätzen sowie in ein grundlegendes Instance Segmentation Verfahren geben, das als Vergleich für das ausgewählte Verfahren dienen wird.  Zentrale Aufgabe wird die Analyse der Funktionsweise des Instance Segmentation Verfahrens auf den neuen Daten sein und in einem zweiten Schritt die Instance Segmentation für diesen Datensatz zu optimieren.
		  	<br />
		  	Für die Projektgruppe stehen zwei Instance Segmentation Ansätze zur Auswahl:
		  	<ol>
		  		<li> <a href="https://arxiv.org/pdf/1911.06667.pdf">Lee, Youngwan, and Jongyoul Park. "Centermask: Real-time anchor-free instance segmentation." 2020.</a> </li>
		  		<li> <a href="https://arxiv.org/pdf/1912.05070.pdf">Wang, Shaoru, et al. "Rdsnet: A new deep architecture forreciprocal object detection and instance segmentation." 2020.</a></li>
		  	</ol>

		  </font></p>
 </td>
 </tr>
</tbody></table>

<br><br>

<!-- ============================================================================ -->

<p>
</p><table id="Feb2017" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
 <td>
 	<img src="../Images/Labs/PG_Wissenstransfer-v2.png" alt="" width="300" border="0">
 </td>
 <td width="40"></td>
 <td> <p align="justify">
      <font face="arial,helvetica,sans-serif">
      <b>Wissenstransfer für Entscheidungswälder (2 Themen / 2 Projektgruppenteilnehmende)</b></font></p><font face="arial,helvetica,sans-serif">
<!-- ============================================================================ -->
      </font>
      <p align="justify">
		  <font face="arial,helvetica,sans-serif">
		  	Satellitenbilder sind eine verlässliche und kostengünstige Datenquelle zur Ertragsprognose. Ziel dieser Projektgruppe ist das Untersuchen verschiedener Verfahren für einen Wissenstransfer zwischen der Ertragsprognose in verschiedenen Regionen.
			<br />
            In der Arbeitsgruppe hatten wir bereits großen Erfolg dabei, Erträge von Sojabohnen in den USA vorherzusagen. Dies ist einer der größten verfügbaren Datensätze zum Thema Ertragsprognose. Als erster Schritt beider Projektarbeiten soll der bestehende Python Code benutzt werden, um ein Baseline-Modell zur Ertragsprognose von Erträgen von Sojabohnen in Argentinien zu entwickeln. Ausgehend davon soll dann getestet werden, ob ein Wissenstransfer von dem in den USA gelerntem Modell auf das Prognosemodell in Argentinien eine Verbesserung zeigt. 
			<br />
		  	Für die Projektgruppe stehen zwei Arbeitspakete  zur Auswahl:
		  	<ol>
		  		<li> Anwenden und Evaluieren des Ansatzes von <a href="https://arxiv.org/abs/1511.01258v1">Segev et al. </a> basierend auf einem bestehenden <a href="https://github.com/atiqm/Transfer_DT">Github repository. </a> </li>
		  		<li> Implementieren und Evaluieren von vier kleinen Lösungen des gegebenen Problems: 
					<ol>
						<li>Reduzieren der Merkmale im Eingaberaum auf Basis der Feature Importances des bereits gelernten Modells</li>
						<li>Gewichtetes Sampling der Merkmale bei der gemäß der gelernten  Feature Importances bei Anwendung von XGBoost</li>
						<li>Anwendung von Vegetation Indices zum Verkleinern des Eingaberaums</li>
						<li>Anwendung klassischer Verfahren zur Dimensionsreduktion, z.B. PCA</li>
					</ol>
				</li>
		  	</ol>
			Beide Arbeiten ermöglichen bei erfolgreicher Bearbeitung eine thematische Fortsetzung zu einer Abschlussarbeit.

		  </font></p>
 </td>
 </tr>
</tbody></table>

<br><br>

<!-- ============================================================================ -->

Termine:
<ul>
<li> Priolisten der Interessierten per Email mit Betreff "Prios PG" bis Fr, 16.09.2022, 22 Uhr</li>
<li> Zuordnungsmitteilung bis Di, 20.09.2022, 18 Uhr </li>
<li> 2-seitig. Expose (Ziel,Daten und Methoden,Zeitplanung) bis Di, 27.09.2022, 14 Uhr</li>
<li> Start der wöchentl. PG-Jour Fixe: Di, 11.10.2022, 14.00 s.t. mit Präsentationen der ersten (Zwischen-)Ergebnisse </li> 
</ul>
<br><br>

<!-- ============================================================================ -->
		<hr size="5" color="#0060AE">  <!--  Trennlinie -->
		
		<table width="90%" cellspacing="20" cellpadding="0" border="0" align="center">
			<tbody><tr>
				<td valign="top" align="left">
					<a href="http://vsteinhage.github.io/index.html">Home</a></td>
				<td valign="top" align="left">
					<a href="http://vsteinhage.github.io/HTML/news.html">News</a></td>
				<td valign="top" align="left">
					<a href="http://vsteinhage.github.io/HTML/teaching.html">Teaching</a></td>
				<td valign="top" align="left">
					<a href="http://vsteinhage.github.io/HTML/projects.html">Projects</a></td>
				<td valign="top" align="left">
					<a href="http://vsteinhage.github.io/HTML/publications.html">Publications</a></td>
				<td valign="top" align="left">
					<a href="http://vsteinhage.github.io/HTML/team.html">Team</a></td>
			</tr>
		</tbody></table>		

		<hr size="5" color="#0060AE">  <!--  Trennlinie -->
		<br><br>

<!-- ============================================================================ -->
		
 </div> <!-- End inhalt -->

<!-- ============================================================================ -->

    <div id="footnote">
      <a href="http://www.uni-bonn.de/"><img id="logo_uni" src="Informatik%204%20-%20Intelligent%20Vision%20Systems%20Group_files/logo_uni.png" name="logo_uni" height="49"></a> 
<!-- 
-->
      <a href="http://net.cs.uni-bonn.de/start/"><img id="logo_cs4" src="Informatik%204%20-%20Intelligent%20Vision%20Systems%20Group_files/logo_cs4_100.png" name="logo_cs4" height="49"></a>

    </div><!-- End footnote-->

<!-- ============================================================================ -->

</div> <!-- End wrapper -->
<div id="spacer">



</div></body>

</html>
