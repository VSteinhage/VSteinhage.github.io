<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Informatik 4 - Intelligent Vision Systems Group</title>
    <!-- Hier das zust&auml;ndinge CSS-->
    <link href="../CSS/style.css" rel="stylesheet" type="text/css" media="screen">
    <!--[if lte IE 6]>
  <style type="text/css">
   @import url(../CSSie5-6.css);
   </style>
   <![endif]-->
<script type="text/javascript" async="" src="Informatik%204%20-%20Intelligent%20Vision%20Systems%20Group_files/nc.js"></script></head>

<body id="teaching">

<div id="wrapper">

<!-- ============================================================================ -->

    <div id="kopfbereich">
		<a href="http://net.cs.uni-bonn.de/wg/intelligent-vision-systems/">
            	   </a><h3><a href="http://net.cs.uni-bonn.de/wg/intelligent-vision-systems/"><img id="logogruppe" src="../Images/Logos/IVS-4-100.png" height="50"></a></h3>
<!--
		<a href="http://net.cs.uni-bonn.de/wg/intelligent-vision-systems/">
            	   <p>Intelligent Vision Systems Group</p></a>
-->
		<a href="http://net.cs.uni-bonn.de/start/">
		   </a><h3><a href="http://net.cs.uni-bonn.de/start/"><img id="logo_cs4" src="../Images/Logos/logo_cs4_100.png" height="50"></a></h3>
    </div><!--End kopfbereich-->

<!-- ============================================================================ -->

	<div id="inhalt">

		<h3>BA-INF 051 - Projektgruppe Intelligente Sehsysteme</h3>

		<ul class="pre_teaching">
			<li class="no_ein"><p>PD Dr. Volker Steinhage</p></li>
			<li class="no_ein"><p>Dienstags, 14-16 Uhr, Seminarraum 1.012 </p></li>
			<li class="no_ein">
				<p>
					Vorbesprechung: Freitag, 17. März. 2023, 15.00 Uhr via <a href="https://bbb.informatik.uni-bonn.de/b/vol-ung-npn">https://bbb.informatik.uni-bonn.de/b/vol-ung-npn</a>
					<!--						<script TYPE="text/javascript" LANGUAGE="JavaScript">
							<!-- den Code vor Browsern verstecken, die kein JavaScript koennen
							var prefix = 'steinhage'; var domain = 'cs.uni-bonn.de';
							document.write('<a href=mailto:' + prefix + '@' + domain + '>');
							document.write(prefix + '@' + domain + '</'+'a>');
							// Ende des Versteckens vor alten Browsern -->


				</p>
			</li>
		</ul>
		<br />

		<!-- ============================================================================ -->
		<p>
			<b>Themen</b>:
			<!-- ============================================================================ -->
			<!-- ============================================================================ -->



		</p>



		<p>
		</p><table id="Feb2017" cellspacing="0" cellpadding="0">
			<tbody>
				<tr valign="top">
					<td>
						<img src="../Images/Labs/InstSeg.png" alt="" width="300" border="0" />
					</td>
					<td width="40"></td>
					<td>
						<p align="justify">
							<font face="arial,helvetica,sans-serif">
								<b>Transfer Learning für Instance Segmentation auf Tag- und Nachtvideos (2 Themen / 2 Projektgruppenteilnehmende)</b>
							</font>
						</p><font face="arial,helvetica,sans-serif">
							<!-- ============================================================================ -->
						</font>
						<p align="justify">
							<font face="arial,helvetica,sans-serif">
								Um Tierpopulationen im Wildlife Monitoring zu überwachen, werden Kamerafallen eingesetzt, die sowohl Videos bei Tag als auch bei Nacht erstellen. Da für Nachtaufnahmen normalerweise nur Infrarot- bzw. Grauwertaufnahmen zur Verfügung stehen, ist die Detektion von Tieren in diesen Fällen oftmals schwieriger als bei Tag.
								<br />
								In dieser Projektgruppe soll untersucht werden, wie die exakten Konturen von Tieren bei Nacht mit Hilfe der sogenannten Instance Segmentation ermittelt werden können. Zur Lösung dieser Aufgabe soll nicht nur das Training der Neuronalen Netzwerke auf Nachtvideos untersucht werden, sondern auch die Möglichkeit mit Hilfe von Transfer Learning die Detektionsfähigkeit der Verfahren für Tag- auf Nachtvideos zu übertragen.
								Die Trainingsdaten stammen dabei aus einem aktuellen Forschungsprojekt. Die Implementierung wird mit Python und PyTorch durchgeführt. Neben der Implementierung des jeweils ausgewählten Instance Segmentation Verfahrens wird die Annotation von Datensätzen betrachtet sowie ein grundlegendes Instance Segmentation Verfahren implementiert, das als Vergleich für das ausgewählte Verfahren dienen wird.
								<br />
								Für die Projektgruppe stehen folgende Instance Segmentation Ansätze zur Auswahl:

								<ol>
									<li> <a href="https://arxiv.org/pdf/1911.06667.pdf">Lee, Youngwan, and Jongyoul Park. "Centermask: Real-time anchor-free instance segmentation." 2020.</a> </li>
									<li> <a href="https://arxiv.org/pdf/2212.02773.pdf">Gu, Zhangxuan, et al. "DiffusionInst: Diffusion Model for Instance Segmentation." 2022.</a></li>
								</ol>

							</font>
						</p>
					</td>
				</tr>
			</tbody>
		</table>

		<br /><br />

		<!-- ============================================================================ -->

		<p>
		</p><table id="Feb2017" cellspacing="0" cellpadding="0">
			<tbody>
				<tr valign="top">
					<td>
						<img src="../Images/Labs/Annotation.png" alt="" width="300" border="0" />
					</td>
					<td width="40"></td>
					<td>
						<p align="justify">
							<font face="arial,helvetica,sans-serif">
								<b>Optimierung eines interaktiven Annotationstools für die Nutzung für Videodaten</b>
							</font>
						</p><font face="arial,helvetica,sans-serif">
							<!-- ============================================================================ -->
						</font>
						<p align="justify">
							<font face="arial,helvetica,sans-serif">
								Datenannotation ist für das Training von Neuronalen Netzwerken unerlässlich. Insbesondere wenn neue Fragestellungen in Forschungsgebieten untersucht werden sollen, existieren meist keine bereits für diese Zielsetzung vorgesehenen Datensätze. Eine vollständige Automatisierung des Annotationsprozesses ist häufig nicht möglich.
								Um eine exakte Groundtruth zu erstellen sind daher häufig semi-automatische Ansätze die beste Wahl, bei denen man mit einer KI-Unterstützung interaktiv die Annotationen erstellen kann.sen in hinsicht auf Präzision sowie untersuchung verschiedener Optical Flow Architekturen.

								<br />
								Das Ziel dieser Projektgruppe ist es, ein interaktives Annotationsverfahren für die Instance Segmentation von Videos aus dem Wildlife Monitoring zu optimieren. Dafür stehen zwei in der Praxis bereits erprobte Interaktive Ansätze, die bildbasiert arbeiten, als Baseline zur Verfügung.
								Neben der Verallgemeinerung von Bild- auf Videodaten steht hier die Untersuchung der Fähigkeit der Verfahren mit Nachtaufnahmen umzugehen, die eine wichtige Datenquelle für das Wildlife Monitoring darstellen.

								<ol>
									<li> <a href="https://arxiv.org/pdf/2210.11006.pdf">Liu, Q., Xu, Z., Bertasius, G., & Niethammer, M. "SimpleClick: Interactive Image Segmentation with Simple Vision Transformers." 2022.</a> </li>
									<li> <a href="https://arxiv.org/pdf/2102.06583.pdf">Sofiiuk, K., Petrov, I. A., & Konushin, A. "Reviving iterative training with mask guidance for interactive segmentation." 2022.</a></li>
								</ol>

							</font>
						</p>
					</td>
				</tr>
			</tbody>
		</table>

		<br /><br />

		<!-- ============================================================================ -->

		<p>
		</p><table id="Feb2017" cellspacing="0" cellpadding="0">
			<tbody>
				<tr valign="top">
					<td>
						<img src="../Images/Labs/Temperature.PNG" alt="" width="300" border="0" />
					</td>
					<td width="40"></td>
					<td>
						<p align="justify">
							<font face="arial,helvetica,sans-serif">
								<b>Vergleichende Auswertung von Thermaler Zeit und Kalenderzeit für die Ernteertragsprognose basierend auf Satellitenbildern</b>
							</font>
						</p><font face="arial,helvetica,sans-serif">

						</font>
						<p align="justify">
							<font face="arial,helvetica,sans-serif">
								Satellitenbilder eignen sich hervorragend zur Prädiktion von Ernteerträgen von verschiedenen Nutzpflanzen 
								weltweit <a href="https://arxiv.org/abs/2208.12633">1</a>. Die Bilder sind kostenlos und weltweit erhältlich und können somit einen Beitrag leisten, 
								Machine Learning Barrierefrei zu gestalten. Unklar ist, wie die Zeitreihe von Daten am besten verarbeitet wird. 
								Der Zeitraum und die Granularität in der Spektrale Reflektionen und Temperaturen über den Feldern erfasst 
								werden müssen, werden meistens als Hyperparameter von Menschen festgelegt. Eine automatisierte Lösung kann 
								durch Thermale Zeiterfassung erfolgen <a href="https://www.researchgate.net/profile/Gregory_Duveiller/publication/235968241_Using_Thermal_Time_and_Pixel_Purity_for_Enhancing_Biophysical_Variable_Time_Series_An_Interproduct_Comparison/links/0c960516bbda2b7e7f000000.pdf">2</a>. 
								Grundlage ist, dass sich verschiedene Nutzpflanzen nur an Tagen 
								mit einer gewissen Mindesttemperatur konstant weiterentwickeln. Im Zuge dieser Projektarbeit sollen beide 
								Ansätze miteinander vergleichend ausgewertet werden. Es kann auf bestehendem Python Code zur Verarbeitung 
								der Satellitenbilder und zum trainieren von Ernteertragsprognosen via 
								Extreme Gradient Boosting (XGBoost) <a href="https://dl.acm.org/doi/pdf/10.1145/2939672.2939785">3</a> aufgebaut werden. Generelle Python Kenntnisse sind also von Vorteil. 
							</font>
						</p>
					</td>
				</tr>
			</tbody>
		</table>

		<br /><br />

		


		<!-- ============================================================================ -->
		Termine:
		<ul>
			<li> Prioliste per Email mit Betreff "Prios PG" bis So, 19.03.2023, 22 Uhr</li>
			<li> Zuordnungsmitteilung bis Mo, 20.03.2023, 20 Uhr </li>
			<li> 2-seitig. Expose (Ziel,Daten und Methoden,Zeitplanung) bis Fr, 31.03.2023, 14 Uhr</li>
			<li> Start der wöchentl. PG-Jour Fixe: Di, 04.04.2023, 14.00 s.t. mit Präsentationen der ersten (Zwischen-)Ergebnisse </li>
		</ul>
		<br /><br />

		<!-- ============================================================================ -->
		<hr size="5" color="#0060AE" />  <!--  Trennlinie -->

		<table width="90%" cellspacing="20" cellpadding="0" border="0" align="center">
			<tbody>
				<tr>
					<td valign="top" align="left">
						<a href="http://vsteinhage.github.io/index.html">Home</a>
					</td>
					<td valign="top" align="left">
						<a href="http://vsteinhage.github.io/HTML/news.html">News</a>
					</td>
					<td valign="top" align="left">
						<a href="http://vsteinhage.github.io/HTML/teaching.html">Teaching</a>
					</td>
					<td valign="top" align="left">
						<a href="http://vsteinhage.github.io/HTML/projects.html">Projects</a>
					</td>
					<td valign="top" align="left">
						<a href="http://vsteinhage.github.io/HTML/publications.html">Publications</a>
					</td>
					<td valign="top" align="left">
						<a href="http://vsteinhage.github.io/HTML/team.html">Team</a>
					</td>
				</tr>
			</tbody>
		</table>

		<hr size="5" color="#0060AE" />  <!--  Trennlinie -->
		<br /><br />

		<!-- ============================================================================ -->

	</div> <!-- End inhalt -->

<!-- ============================================================================ -->

    <div id="footnote">
      <a href="http://www.uni-bonn.de/"><img id="logo_uni" src="Informatik%204%20-%20Intelligent%20Vision%20Systems%20Group_files/logo_uni.png" name="logo_uni" height="49"></a> 
<!-- 
-->
      <a href="http://net.cs.uni-bonn.de/start/"><img id="logo_cs4" src="Informatik%204%20-%20Intelligent%20Vision%20Systems%20Group_files/logo_cs4_100.png" name="logo_cs4" height="49"></a>

    </div><!-- End footnote-->

<!-- ============================================================================ -->

</div> <!-- End wrapper -->
<div id="spacer">



</div></body></html>