 <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<meta http-equiv="Content-Type" content="text-html" />
    <title>Informatik 4 - Intelligent Vision Systems Group</title>
    <!-- Hier das zust&auml;ndinge CSS-->
    <link href="../CSS/style.css" rel="stylesheet" type="text/css" media="screen" />
    <!--[if lte IE 6]>
  <style type="text/css">
   @import url(../CSSie5-6.css);
   </style>
   <![endif]-->
</head>

<body id="teaching">

<div id="wrapper">

<!-- ============================================================================ -->

    <div id="kopfbereich">
		<a href="http://net.cs.uni-bonn.de/wg/intelligent-vision-systems/">
            	   <h3><img id="logogruppe" src="../Images/Logos/IVS-4-100.png" height="50" /></a></h3>
<!--
		<a href="http://net.cs.uni-bonn.de/wg/intelligent-vision-systems/">
            	   <p>Intelligent Vision Systems Group</p></a>
-->
		<a href="http://net.cs.uni-bonn.de/start/">
		   <h3><img id="logo_cs4" src="../Images/Logos/logo_cs4_100.png" height="50" /></a></h3>
    </div><!--End kopfbereich-->

<!-- ============================================================================ -->

 <div id="inhalt">

<h3>BA-INF 051 - Projektgruppe Intelligente Sehsysteme</h3>

    <ul class="pre_teaching">
	    <li class="no_ein"><p>PD Dr. Volker Steinhage</p></li>
	    <li class="no_ein"><p>Dienstags, 14-16 Uhr, online </p></li>
	    <li class="no_ein"><p>Vorbesprechung: Donnerstag, 18. März 2020, 16.15 Uhr via <a href="https://bbb.informatik.uni-bonn.de/b/vol-x2d-g4x">https://bbb.informatik.uni-bonn.de/b/vol-x2d-g4x</a>
<!--						<script TYPE="text/javascript" LANGUAGE="JavaScript">
							<!-- den Code vor Browsern verstecken, die kein JavaScript koennen
							var prefix = 'steinhage'; var domain = 'cs.uni-bonn.de';
							document.write('<a href=mailto:' + prefix + '@' + domain + '>');
							document.write(prefix + '@' + domain + '</'+'a>');
							// Ende des Versteckens vor alten Browsern -->
						</script>

		</p></li>
    </ul>
<br />

<!-- ============================================================================ -->
<p>
<b>Themen</b>:
<!-- ============================================================================ -->

<!-- ============================================================================ -->
<p>
<table id="Feb2017" cellspacing="0" cellpadding="0">
<tr valign="top" >
 <td>
	<div>
 	<img src="../Images/Labs/SyntPicsNVIDEA-V3-top.jpg" width="300" border="0" style=display:block
		alt="InstSeg">

	<img src="../Images/Labs/white.jpg" width="300" border="0" style=display:block
		alt="InstSeg">	

	<img src="../Images/Labs/SyntPicsNVIDEA-V3-bot.jpg" width="300" border="0" style=display:block
		alt="InstSeg">
	<img src="../Images/Labs/white.jpg" width="300" border="0" style=display:block
		alt="InstSeg">
	<img src="../Images/Labs/SyntPicsDear.jpg" width="300" border="0" style=display:block
		alt="InstSeg">
	</div>
 </td>
 <td width="40"></td>
 <td> <p align="justify">
      <font face="arial,helvetica,sans-serif">
      <b>Instance Rendering von synthet. Daten für Supervised Deep Learning</b></p>
<!--  ============================================================================ -->
      </font>
      <p align="justify">
      <font face="arial,helvetica,sans-serif">

	Supervised Deep Learning erfordert große Mengen von annotierten Trainingsdaten. Häufig fehlen entsprechend große Mengen, 
	sodass annotierten Trainingsdaten synthetisch erzeugt werden. In der Computer Vision steht dabei das Rendering von 3D-Szenen 
	im Vordergrund. Neben regulären RGB-Bildern werden Segmentierungsmasken, Instanzenmasken oder Tiefenbilder gerendert. 
	Zielsetzung der Arbeiten ist 
	(1) die Generierung synthetischer Bildsequenzen von animierten Wildtiermodellen mit einer Rendering Engine, 
	(2) das Training einer Deep-Learning-Architektur sowie 
	(3) die Evaluierung der Ergebnisse. 



	Zwingend erforderlich sind Kenntnisse in Computergrafik, Modellierung und Scripting mit Blender und Python oder mit 
	Unreal Engine und C++. Begründete Alternativvorschläge für Rendering-Engines und Scripting-Umgebungen sind möglich. 
	Hilfreich aber nicht erforderlich ist Erfahrung mit Deep Learning und Frameworks wie PyTorch oder Tensorflow. </p>

	Oben und mittig: RGB-Bild, Instanzmaske, Tiefenbild, Pose (Quelle: NVIDIA).</p>

	Unten: Synthetisches Rendering eines Rehs mit Objektmaske, RGB-Bild, Tiefenbild 

      </font>
      <p align="justify">
      <font face="arial,helvetica,sans-serif">
 </td>
 </tr>
</table>
<br><br>

<!-- ============================================================================ -->
<p>
<table id="Feb2017" cellspacing="0" cellpadding="0">
<tr valign="top" >
 <td>
	<div>
 	<img src="../Images/Labs/SegmentEveryThing.jpg" width="300" border="0" style=display:block
		alt="InstSeg">
	</div>
 </td>
 <td width="40"></td>
 <td> <p align="justify">
      <font face="arial,helvetica,sans-serif">
      <b>Learning to Segment Every Thing with Few Annotations</b></p>
<!--  ============================================================================ -->
      </font>
      <p align="justify">
      <font face="arial,helvetica,sans-serif">

	Beim Wildtiermonitoring stellt die Detektion und Lokalisierung von Tieren eine anspruchsvolle Aufgabe dar, 
	die mit Methoden des Supervised Deep Learnings angegangen wird. Dafür sind große Mengen von Trainingsdaten 
	erforderlich, die per Hand annotiert werden müssen. Die Lokalisierung kann dabei entweder mittels Bounding-Boxes 
	(grob) oder mittels Instanzmasken (fein) erfolgen. Während erstere mit relativ geringem Aufwand annotiert werden 
	können, so ist die Annotation bei Instanzmasken sehr viel aufwändiger. 

	Die Methode von <a href="http://ronghanghu.com/seg_every_thing/">Hu et al. 2018</a> erlaubt 
	es hingegen, auf solche manuell annotierten Instanzmasken zugunsten von Bounding-Boxes zu verzichten. Trotzdem 
	können mittels dieser Methode Instanzmasken erzeugt werden, indem das Wissen, welches zur Vorhersage von 
	Bounding-Boxes genutzt wird, auf die Vorhersage der Instanzmasken übertragen wird. Das Ziel ist die Anwendung der 
	Methodik von Hu et al. 2018 auf mehrere Datensätze in der Domäne des Wildtiermonitorings. 

	Falls für den jeweiligen Datensatz keine manuell annotierten Bounding-Boxes zum Training zur Verfügung stehen, soll ein robustes 
	Modell zur automatisierten Erzeugung ebd. (<a href="https://arxiv.org/abs/1907.06772">Beery et al. 2019)</a> eingesetzt werden. 
	Ein weiteres Ziel ist die Fusionierung der Ansätze von Hu et al. 2018 und  Beery et al. 2019 in einem grafischen Annotationstool. </p>

	Erforderlich sind grundlegende Kenntnisse im Bereich des Deep Learnings und in Python.</p>

	Bildquelle: <a href="https://emammal.si.edu/">eMammal</a>

      </font>
      <p align="justify">
      <font face="arial,helvetica,sans-serif">
 </td>
 </tr>
</table>
<br><br>

<!-- ============================================================================ -->
<!-- ============================================================================ -->

<p>
<table id="Feb2017" cellspacing="0" cellpadding="0">
<tr valign="top" >
 <td>
 	<img src="../Images/Labs/ReId.jpg" width="300" border="0" 
		alt="InstAnno">
 </td>
 <td width="40"></td>
 <td> <p align="justify">
      <font face="arial,helvetica,sans-serif">
      <b>Re-Identification (2 Themen / 2 Projektgruppenteilnehmende)</b></p>
<!-- ============================================================================ -->
      </font>
      <p align="justify">
      <font face="arial,helvetica,sans-serif">

	Das Ziel ist die Analyse und die Anpassung einer Implementierung eines modernen Re-Identification Ansatzes, der auf Künstlichen Neuronalen Netzen basiert. 
	Dabei wird mit Trainingsdaten aus einem aktuellen Forschungsprojekt gearbeitet. Bevor der Re-Identification-Algorithmus implementiert wird, gibt es eine 
	kurze Einarbeitungsphase in Python und PyTorch, auf denen beide Ansätze basieren. 

	Das Ziel eines Re-Identification-Algorithmus ist es, in einem Bild oder Video Objekte, z.B. individuelle Personen, wiederzuerkennen. Es geht also nicht nur darum, 
	ein Objekt allgemein zu detektieren und zu klassifizieren, sondern jedes einzelne Individuum zu unterscheiden. Die angegebenen Re-Identification Verfahren basieren 
	auf einer Personenwiedererkennung. 
	Hier soll untersucht werden, ob sich die Verfahren auf Videodaten von Wildtieren übertragen lassen. Mögliche Probleme und Unterschiede bei der Anwendung auf Wildtierdaten 
	sollen herausgearbeitet werden.</p>
	
	Es gibt zwei Re-Identification Ansätze zur Auswahl für die Teilnehmenden:</p>

	<ol>
  	<li> Herzog, Fabian, et al. "Lightweight Multi-Branch Network for Person Re-Identification." arXiv preprint arXiv:2101.10774 (2021). </p>

	<li> Quispe, Rodolfo, and Helio Pedrini. "Top-DB-Net: Top DropBlock for Activation Enhancement in Person Re-Identification." arXiv preprint arXiv:2010.05435 (2020). </p>
	</ol>

 </td>
 </tr>
</table>

<br><br>

<!-- ============================================================================ -->

<p>
<table id="Feb2017" cellspacing="0" cellpadding="0">
<tr valign="top" >
 <td>
 	<img src="../Images/Labs/Weng.jpg" width="300" border="0" 
		alt="3DReco">
 </td>
 <td width="40"></td>
 <td> <p align="justify">
      <font face="arial,helvetica,sans-serif">
      <b>3D Classification and Tracking</b>
<!-- ============================================================================ -->
      </font>
      <p align="justify">
      <font face="arial,helvetica,sans-serif">

	Ziel ist, die mehrstufige Architektur von <a href="https://arxiv.org/pdf/1907.03961.pdf">Weng et al. 2020</a> zunächst umzusetzen. 
	Hierbei sollen anfangs 3D Punktwolken klassifiziert werden. Daraufhin sollen solche erkannten Objekte des
	Kalman-Filters verfolgt werden. Dazu sollen Ergebnisse visualisiert und evaluiert werden. Zuletzt
	sollten einzelne Systeme der Architektur optimiert werden bzw. auf neue Daten transferiert
	werden.</p>
	Hierzu sollten (sehr-)gute Kenntnisse in Python (+ eventuell Cpp) und DL-Frameworks wie Pytorch
	vorliegen. Zudem müssen grundlegende Konzepte der KI und Neuronaler Netze, aber auch
	klassische Algorithmen wie der Kalman-Filter, verstanden sein.</p>

	Bildquelle: <a href="https://arxiv.org/pdf/1907.03961.pdf">Weng et al. 2020</a>

 </td>
 </tr>
</table>

<br><br>

<!-- ============================================================================ -->


<br><br>

Termine:
<ul>
<li> Prioliste per Email bis Mo, 22.03.2021, 10 Uhr</li>
<li> Zuordnungsmitteilung bis Mi, 24.03.2021, 18 Uhr </li>
<li> 2-seitig. Expose (Ziel,Daten und Methoden,Zeitplanung) bis Mi, 31.03.2021, 14 Uhr </li>
<li> Start der wöchentl. PG-Jour Fixe: Di, 13.04.2021, 14.00 s.t. per BBB mit Präsentationen der ersten Zwischenergebnisse </li>
</ul>
<br><br>

<!-- ============================================================================ -->
		<hr color="#0060AE" size="5" >  <!--  Trennlinie -->
		
		<table width="90%" border="0" align="center" cellspacing="20" cellpadding="0">
			<tr>
				<td valign="top" align="left">
					<a href="../index.html">Home</a></td>
				<td valign="top" align="left">
					<a href="news.html">News</a></td>
				<td valign="top" align="left">
					<a href="teaching.html">Teaching</a></td>
				<td valign="top" align="left">
					<a href="projects.html">Projects</a></td>
				<td valign="top" align="left">
					<a href="publications.html">Publications</a></td>
				<td valign="top" align="left">
					<a href="team.html">Team</a></td>
			</tr>
		</table>		

		<hr color="#0060AE" size="5" >  <!--  Trennlinie -->
		<br><br>

<!-- ============================================================================ -->
		
 </div> <!-- End inhalt -->

<!-- ============================================================================ -->

    <div id="footnote">
      <a href="http://www.uni-bonn.de"><img id="logo_uni" src="../Images/Logos/logo_uni.png" height="49" name="logo_uni" /></a> 
<!-- 
-->
      <a href="http://net.cs.uni-bonn.de/start/"><img id="logo_cs4" src="../Images/Logos/logo_cs4_100.png" height="49" name="logo_cs4" /></a>

    </div><!-- End footnote-->

<!-- ============================================================================ -->

</div> <!-- End wrapper -->
<div id="spacer"/>
</body>

</html>
