<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Informatik 4 - Intelligent Vision Systems Group</title>
    <!-- Hier das zust&auml;ndinge CSS-->
    <link href="Informatik%204%20-%20Intelligent%20Vision%20Systems%20Group_files/style.css" rel="stylesheet" type="text/css" media="screen">
    <!--[if lte IE 6]>
  <style type="text/css">
   @import url(../CSSie5-6.css);
   </style>
   <![endif]-->
<script type="text/javascript" async="" src="Informatik%204%20-%20Intelligent%20Vision%20Systems%20Group_files/nc.js"></script></head>

<body id="teaching">

<div id="wrapper">

<!-- ============================================================================ -->

    <div id="kopfbereich">
		<a href="http://net.cs.uni-bonn.de/wg/intelligent-vision-systems/">
            	   </a><h3><a href="http://net.cs.uni-bonn.de/wg/intelligent-vision-systems/"><img id="logogruppe" src="Informatik%204%20-%20Intelligent%20Vision%20Systems%20Group_files/IVS-4-100.png" height="50"></a></h3>
<!--
		<a href="http://net.cs.uni-bonn.de/wg/intelligent-vision-systems/">
            	   <p>Intelligent Vision Systems Group</p></a>
-->
		<a href="http://net.cs.uni-bonn.de/start/">
		   </a><h3><a href="http://net.cs.uni-bonn.de/start/"><img id="logo_cs4" src="Informatik%204%20-%20Intelligent%20Vision%20Systems%20Group_files/logo_cs4_100.png" height="50"></a></h3>
    </div><!--End kopfbereich-->

<!-- ============================================================================ -->

 <div id="inhalt">

<h3>BA-INF 051 - Projektgruppe Intelligente Sehsysteme</h3>

    <ul class="pre_teaching">
	    <li class="no_ein"><p>PD Dr. Volker Steinhage</p></li>
	    <li class="no_ein"><p>Dienstags, 14-16 Uhr, online </p></li>
	    <li class="no_ein"><p>Vorbesprechung: Montag, 21. März. 2022, 15.00 Uhr via <a href="https://bbb.informatik.uni-bonn.de/b/vol-ung-npn">https://bbb.informatik.uni-bonn.de/b/vol-ung-npn</a>
<!--						<script TYPE="text/javascript" LANGUAGE="JavaScript">
							<!-- den Code vor Browsern verstecken, die kein JavaScript koennen
							var prefix = 'steinhage'; var domain = 'cs.uni-bonn.de';
							document.write('<a href=mailto:' + prefix + '@' + domain + '>');
							document.write(prefix + '@' + domain + '</'+'a>');
							// Ende des Versteckens vor alten Browsern -->
						

		</p></li>
    </ul>
<br>

<!-- ============================================================================ -->
<p>
<b>Themen</b>:
<!-- ============================================================================ -->

<!-- ============================================================================ -->
</p><p>
</p><table id="Feb2017" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
 <td>
	<div>
 	<img src="Informatik%204%20-%20Intelligent%20Vision%20Systems%20Group_files/rgbd_presence_absence.png" style="display:block" alt="" width="300" border="0">
	</div>
 </td>
 <td width="40"></td>
 <td> <p align="justify">
      <font face="arial,helvetica,sans-serif">
      <b>Animal Presence-Absence mittels RGB-D-Daten</b></font></p><font face="arial,helvetica,sans-serif">
<!--  ============================================================================ -->
      </font>
      <p align="justify">
      <font face="arial,helvetica,sans-serif">
		Zur Beobachtung von Tieren werden häufig Kamerafallen eingesetzt, die mittels Bewegungsmelder auslösen. Diese Bewegungsmelder lösen jedoch auch häufig ohne Bewegung aus, was zu viel überflüssigem Bildmaterial führt. Hier soll dieses überflüssige Bildmaterial aussortiert werden. Dazu soll jedes Bild mittels eines Deep-Learning-Ansatzes als “enthält Tier” oder “enthält kein Tier” klassifiziert werden. Dabei steht als Eingabe nicht nur ein reguläres Intensitätsbild, sondern auch ein Tiefenbild, welches für jeden Pixel die Distanz zur Kamera kodiert, zur Verfügung. Es soll evaluiert werden inwiefern die zusätzlichen Tiefeninformationen zu einer Verbesserung der Erkennung beitragen. Als Ground Truth sollen die Ergenisse des “MegaDetectors” (<a href="https://arxiv.org/abs/1907.06772">Beery et al. 2019</a>) dienen (Knowledge Distillation, <a href="https://arxiv.org/abs/1503.02531">Hinton et al. 2015</a>).
	</font></p>
      </font>
      <p align="justify">
      <font face="arial,helvetica,sans-serif">
 </font></p></td>
 </tr>
</tbody></table>
<br><br>

<!-- ============================================================================ -->
<p>
</p><table id="Feb2017" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
 <td>
	<div>
 	<img src="Informatik%204%20-%20Intelligent%20Vision%20Systems%20Group_files/ir2rgb.png" style="display:block" alt="" width="300" border="0">
	</div>
 </td>
 <td width="40"></td>
 <td> <p align="justify">
      <font face="arial,helvetica,sans-serif">
      <b>Kolorierung von Nah-Infrarotbildern</b></font></p><font face="arial,helvetica,sans-serif">
<!--  ============================================================================ -->
      </font>
      <p align="justify">
      <font face="arial,helvetica,sans-serif">
		Wild- und Überwachungskameras verwenden häufig unsichtbare Infrarotbeleuchtung und monochrome Bildsensoren um bei Nacht zumindest Grauwertbilder aufnehmen zu können. Für viele anschließende Aufgaben wie Objekterkennung sind Farbbilder allerdings besser geeignet. Daher gibt es bereits diverse, oft auf Deep Learning basierende Ansätze, um solche Aufnahmen zu kolorieren (<a href="https://arxiv.org/abs/1604.02245">Limmer and Lensch 2016</a>, <a href="https://waseda.pure.elsevier.com/en/publications/infrared-image-colorization-using-a-s-shape-network">Dong et al. 2018</a>, <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/PBVS/Mehri_Colorizing_Near_Infrared_Images_Through_a_Cyclic_Adversarial_Approach_of_CVPRW_2019_paper.pdf">Mehri and Sappa 2019</a>). Diese Ansätze sollen vergleichend Evaluiert und auf die Kolorierung von Wildtieraufnahmen angepasst werden. Hierfür sind gute Python-Kenntnisse erforderlich und Erfahrungen mit PyTorch vorteilhaft.
	  </font></p><p></p><font face="arial,helvetica,sans-serif">

	Bildquelle: <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/PBVS/Mehri_Colorizing_Near_Infrared_Images_Through_a_Cyclic_Adversarial_Approach_of_CVPRW_2019_paper.pdf">Mehri and Sappa 2019</a>

      </font>
      <p align="justify">
      <font face="arial,helvetica,sans-serif">
 </font></p></td>
 </tr>
</tbody></table>
<br><br>

<!-- ============================================================================ -->

<p>
	</p><table id="Feb2017" cellspacing="0" cellpadding="0">
	<tbody><tr valign="top">
	 <td>
		 <img src="Informatik%204%20-%20Intelligent%20Vision%20Systems%20Group_files/visTR.png" alt="" width="300" border="0">
	 </td>
	 <td width="40"></td>
	 <td> <p align="justify">
		  <font face="arial,helvetica,sans-serif">
		  <b>Improved Instance Segmentation in Videos</b></font></p><font face="arial,helvetica,sans-serif">
	<!-- ============================================================================ -->
		  </font>
		  <p align="justify">
			  <font face="arial,helvetica,sans-serif">
		Bei der herkömmlichen Instanz Segmentierung, wird auf dem eingegebenen
 Bild, die Objektklasse sowie eine pro-pixel Objekt-Maskierung bestimmt.
 Während dies eine allgemeine Lösung für herkömmliche Daten ist, kann 
die performanz auf schweren Bildern abfallen. Eine Lösung ist dabei, 
falls vorhanden, die Kontektinformationen eines ganzen Videoclips zu 
verwenden um mehr Informationen außerhalb des aktuellen Bilds 
einzubringen. In diesem Projekt geht es darum zunächst eine kurze 
Einarbeitung in die Netzwerkarchitektur vorzunehmen. Darauf gefolgt, 
soll dann auf einem Infrarot Videostream von Kamerafallen gearbeitet 
werden. Ziel des Projekts ist es erfolgreich auf dem Datensatz und mit 
dem Neuronal Netz zu arbeiten. Dabei soll eine vergleichende Evaluation 
mit aktuell vorliegenden Ergebnissen gemacht werden.

		</font></p><ul><font face="arial,helvetica,sans-serif">
			<li><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Wang_End-to-End_Video_Instance_Segmentation_With_Transformers_CVPR_2021_paper.html">Video Instance Segmentation with Transformers</a></li>
		</font></ul><font face="arial,helvetica,sans-serif">
		Bildquelle: <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Wang_End-to-End_Video_Instance_Segmentation_With_Transformers_CVPR_2021_paper.html"> VisTR </a>
	 </font></td>
	 </tr>
	</tbody></table>
	
	<br><br>
	
	<!-- ============================================================================ -->

<!-- ============================================================================ -->

<p>
	</p><table id="Feb2017" cellspacing="0" cellpadding="0">
	<tbody><tr valign="top">
	 <td>
		 <img src="Informatik%204%20-%20Intelligent%20Vision%20Systems%20Group_files/ritm.png" alt="" width="300" border="0">
	 </td>
	 <td width="40"></td>
	 <td> <p align="justify">
		  <font face="arial,helvetica,sans-serif">
		  <b>Verbesserung des RITM Annotation Tools</b></font></p><font face="arial,helvetica,sans-serif">
	<!-- ============================================================================ -->
		  </font>
		  <p align="justify">
			  <font face="arial,helvetica,sans-serif">
				Im folgenden Thema geht es darum, ein existierendes Instanz-Masken Annotations Tool zu weiter zu entwickeln [1].
				Hierbei soll ein Smart Propagation Module entwickelt werden, dass mittels Optical Flows den Arbeitsaufwand von Video-Annotationen reduzieren soll [2].
				Ziel der Projektgruppe wird zunächst die Umsetzung und Implementierung des gennanten Modules sein. Darauf folgen sollen verschiedene qualitative analysen in hinsicht auf Präzision sowie untersuchung verschiedener Optical Flow Architekturen.

		</font></p><ol><font face="arial,helvetica,sans-serif">
			<li>Reviving Iterative Training with Mask Guidance for Interactive Segmentation <a href="https://arxiv.org/abs/2102.06583">https://arxiv.org/abs/2102.06583</a></li>
			<li>FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks <a href="https://arxiv.org/abs/1612.01925">https://arxiv.org/abs/1612.01925</a></li>
		</font></ol><font face="arial,helvetica,sans-serif">
	 </font></td>
	 </tr>
	</tbody></table>
	
	<br><br>
	
	<!-- ============================================================================ -->


<!-- ============================================================================ -->

<p>
</p><table id="Feb2017" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
 <td>
 	<img src="Informatik%204%20-%20Intelligent%20Vision%20Systems%20Group_files/PGInstSegAbb.jpg" alt="" width="300" border="0">
 </td>
 <td width="40"></td>
 <td> <p align="justify">
      <font face="arial,helvetica,sans-serif">
      <b>Instance Segmentation (2 Themen / 2 Projektgruppenteilnehmende)</b></font></p><font face="arial,helvetica,sans-serif">
<!-- ============================================================================ -->
      </font>
      <p align="justify">
		  <font face="arial,helvetica,sans-serif">
		  	Ziel dieser Projektgruppe ist die Untersuchung eines Instance Segmentation Verfahrens, das auf Künstlichen Neuronalen Netzen basiert. Die Aufgabe einer Instance Segmentation ist es, Objekte in einem Bild oder Video zu detektieren und den genauen Umriss, die Instance Mask dieses Objektes, zu bestimmen. So können Individuen besser voneinander unterschieden werden.
			  <br />
		  Der ausgewählte Ansatz wird auf Trainingsdaten aus einem aktuellen Forschungsprojekt angewendet. Die Implementierung wird mit Python und PyTorch durchgeführt. Neben der Implementierung des Instance Segmentation Verfahrens wird es eine Einarbeitungsphase in die Annotation von Datensätzen sowie in ein grundlegendes Instance Segmentation Verfahren geben, das als Vergleich für das ausgewählte Verfahren dienen wird.  Zentrale Aufgabe wird die Analyse der Funktionsweise des Instance Segmentation Verfahrens auf den neuen Daten sein und in einem zweiten Schritt die Instance Segmentation für diesen Datensatz zu optimieren.
		  	<br />
		  	Für die Projektgruppe stehen zwei Instance Segmentation Ansätze zur Auswahl:
		  	<ol>
		  		<li> <a href="https://arxiv.org/pdf/1911.06667.pdf">Lee, Youngwan, and Jongyoul Park. "Centermask: Real-time anchor-free instance segmentation." 2020.</a> </li>
		  		<li> <a href="https://arxiv.org/pdf/1912.05070.pdf">Wang, Shaoru, et al. "Rdsnet: A new deep architecture forreciprocal object detection and instance segmentation." 2020.</a></li>
		  	</ol>

		  </font></p>
 </td>
 </tr>
</tbody></table>

<br><br>

<!-- ============================================================================ -->

<p>
</p><table id="Feb2017" cellspacing="0" cellpadding="0">
<tbody><tr valign="top">
 <td>
 	<img src="Informatik%204%20-%20Intelligent%20Vision%20Systems%20Group_files/PG_Wissenstransfer.jpg" alt="" width="300" border="0">
 </td>
 <td width="40"></td>
 <td> <p align="justify">
      <font face="arial,helvetica,sans-serif">
      <b>Wissenstransfer für Entscheidungswälder (2 Themen / 2 Projektgruppenteilnehmende)</b></font></p><font face="arial,helvetica,sans-serif">
<!-- ============================================================================ -->
      </font>
      <p align="justify">
		  <font face="arial,helvetica,sans-serif">
		  	Satellitenbilder sind eine verlässliche und kostengünstige Datenquelle zur Ertragsprognose. Ziel dieser Projektgruppe ist das Untersuchen verschiedener Verfahren für einen Wissenstransfer zwischen der Ertragsprognose in verschiedenen Regionen.
			<br />
            In der Arbeitsgruppe hatten wir bereits großen Erfolg dabei, Erträge von Sojabohnen in den USA vorherzusagen. Dies ist einer der größten verfügbaren Datensätze zum Thema Ertragsprognose. Als erster Schritt beider Projektarbeiten soll der bestehende Python Code benutzt werden, um ein Baseline-Modell zur Ertragsprognose von Erträgen von Sojabohnen in Argentinien zu entwickeln. Ausgehend davon soll dann getestet werden, ob ein Wissenstransfer von dem in den USA gelerntem Modell auf das Prognosemodell in Argentinien eine Verbesserung zeigt. 
			<br />
		  	Für die Projektgruppe stehen zwei Arbeitspakete  zur Auswahl:
		  	<ol>
		  		<li> Anwenden und Evaluieren des Ansatzes von <a href="https://arxiv.org/abs/1511.01258v1">Segev et al. </a> basierend auf einem bestehenden <a href="https://github.com/atiqm/Transfer_DT">Github repository. </a> </li>
		  		<li> Implementieren und Evaluieren von vier kleinen Lösungen des gegebenen Problems: 
					<ol>
						<li>Reduzieren der Merkmale im Eingaberaum auf Basis der Feature Importances des bereits gelernten Modells</li>
						<li>Gewichtetes Sampling der Merkmale bei der gemäß der gelernten  Feature Importances bei Anwendung von XGBoost</li>
						<li>Anwendung von Vegetation Indices zum Verkleinern des Eingaberaums</li>
						<li>Anwendung klassischer Verfahren zur Dimensionsreduktion, z.B. PCA</li>
					</ol>
				</li>
		  	</ol>
			Beide Arbeiten ermöglichen bei erfolgreicher Bearbeitung eine thematische Fortsetzung zu einer Abschlussarbeit.

		  </font></p>
 </td>
 </tr>
</tbody></table>

<br><br>

<!-- ============================================================================ -->

Termine:
<ul>
<li> Prioliste per Email mit Betreff "Prios PG" bis Do, 24.03.2022, 22 Uhr</li>
<li> Zuordnungsmitteilung bis Mo, 28.03.2022, 18 Uhr </li>
<li> 2-seitig. Expose (Ziel,Daten und Methoden,Zeitplanung) bis Mo, 04.04.2022, 14 Uhr</li>
<li> Start der wöchentl. PG-Jour Fixe: Mo, 11.04.2022, 14.00 s.t. per BBB mit Präsentationen der ersten Zwischenergebnisse </li>
</ul>
<br><br>

<!-- ============================================================================ -->
		<hr size="5" color="#0060AE">  <!--  Trennlinie -->
		
		<table width="90%" cellspacing="20" cellpadding="0" border="0" align="center">
			<tbody><tr>
				<td valign="top" align="left">
					<a href="http://vsteinhage.github.io/index.html">Home</a></td>
				<td valign="top" align="left">
					<a href="http://vsteinhage.github.io/HTML/news.html">News</a></td>
				<td valign="top" align="left">
					<a href="http://vsteinhage.github.io/HTML/teaching.html">Teaching</a></td>
				<td valign="top" align="left">
					<a href="http://vsteinhage.github.io/HTML/projects.html">Projects</a></td>
				<td valign="top" align="left">
					<a href="http://vsteinhage.github.io/HTML/publications.html">Publications</a></td>
				<td valign="top" align="left">
					<a href="http://vsteinhage.github.io/HTML/team.html">Team</a></td>
			</tr>
		</tbody></table>		

		<hr size="5" color="#0060AE">  <!--  Trennlinie -->
		<br><br>

<!-- ============================================================================ -->
		
 </div> <!-- End inhalt -->

<!-- ============================================================================ -->

    <div id="footnote">
      <a href="http://www.uni-bonn.de/"><img id="logo_uni" src="Informatik%204%20-%20Intelligent%20Vision%20Systems%20Group_files/logo_uni.png" name="logo_uni" height="49"></a> 
<!-- 
-->
      <a href="http://net.cs.uni-bonn.de/start/"><img id="logo_cs4" src="Informatik%204%20-%20Intelligent%20Vision%20Systems%20Group_files/logo_cs4_100.png" name="logo_cs4" height="49"></a>

    </div><!-- End footnote-->

<!-- ============================================================================ -->

</div> <!-- End wrapper -->
<div id="spacer">



</div></body></html>